{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelling overview\n",
    "\n",
    "In this notebook:\n",
    "\n",
    "- Description of the data\n",
    "- Looking at the data\n",
    "- Text pre-processing\n",
    "- Topic Modelling with Gensim\n",
    "- Visualisation of Topic Models with pyLDAvis\n",
    "- Topic coherence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some configuration first\n",
    "\n",
    "The following cell will download some components of the NLTK library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, watch out for deprecation warnings (show them once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='once')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of the data\n",
    "\n",
    "- We're going to use a sub-set of the popular `20newsgroups` dataset\n",
    "- Each document is a newsgroup message\n",
    "- Each document is labelled with the related newsgroup (one newsgroup per document)\n",
    "- There are (surprise!) 20 newsgroups\n",
    "- The newsgroup name tells us about the overall topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = ['comp.sys.mac.hardware',\n",
    "              'rec.autos',\n",
    "              'sci.space',\n",
    "              'misc.forsale',\n",
    "              'talk.politics.guns',\n",
    "              'talk.religion.misc']\n",
    "\n",
    "newsgroups = fetch_20newsgroups(remove=('headers', 'footers', 'quotes'),\n",
    "                                categories=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(newsgroups.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look into the content of a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = newsgroups.data[2]\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_class = newsgroups.target[2]\n",
    "doc_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups.target_names[doc_class]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text pre-processing\n",
    "\n",
    "Gensim expects the input corpus to be a sequence of tokenised documents\n",
    "\n",
    "e.g. a list of lists (documents) of strings (words/tokens)\n",
    "\n",
    "In our first iteration, we're simply tokenising the input data (from doc to words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def preprocess(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "corpus = [preprocess(doc) for doc in newsgroups.data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the term-document matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "id2word = Dictionary(corpus)\n",
    "\n",
    "# Term Document Frequency\n",
    "term_document_matrix = [id2word.doc2bow(text) for text in corpus]\n",
    "\n",
    "# View one document in the term-document matrix\n",
    "print(term_document_matrix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of documents\n",
    "len(term_document_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of unique words (vocabulary size)\n",
    "len(id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View one word\n",
    "id2word[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View word frequency distribution in one document\n",
    "doc = term_document_matrix[0]\n",
    "[(id2word[word_id], freq) for word_id, freq in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train topic model with LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "model = LdaModel(corpus=term_document_matrix,\n",
    "                 id2word=id2word,\n",
    "                 num_topics=10, \n",
    "                 passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we do better?\n",
    "\n",
    "Have a look at the topics extracted in the example above:\n",
    "\n",
    "- does the output make sense?\n",
    "- is the output useful at all?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Better pre-processing\n",
    "\n",
    "Some options to improve pre-processing:\n",
    "\n",
    "- normalisation (e.g. lowercasing)\n",
    "- stop-word removal\n",
    "- punctuation removal\n",
    "\n",
    "Data cleaning is not glamorous, but it can have a big impact on our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "STOP_LIST = set(stopwords.words('english') + list(punctuation))\n",
    "STOP_LIST.update([\"'m\", \"n't\", '``', \"'s\", \"'ll\", \"'re\", '--', \"''\", '\"\"', '...'])\n",
    "STOP_LIST.update(['go', 'get', 'like', 'gon', 'na', 'oh', 'yeah'])\n",
    "\n",
    "def preprocess(text):\n",
    "    return [word.lower() for word in word_tokenize(text) if word.lower() not in STOP_LIST]\n",
    "\n",
    "corpus = [preprocess(doc) for doc in newsgroups.data]\n",
    "id2word = Dictionary(corpus)\n",
    "term_document_matrix = [id2word.doc2bow(text) for text in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model = LdaModel(corpus=term_document_matrix,\n",
    "                id2word=id2word,\n",
    "                num_topics=10,\n",
    "                passes=10)\n",
    "\n",
    "model.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing the extremes of the distribution\n",
    "\n",
    "Zipf's Law - https://en.wikipedia.org/wiki/Zipf%27s_law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = Dictionary(corpus)\n",
    "id2word.filter_extremes(no_below=10, no_above=0.5)\n",
    "term_document_matrix = [id2word.doc2bow(text) for text in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model = LdaModel(corpus=term_document_matrix,\n",
    "                id2word=id2word,\n",
    "                num_topics=10,\n",
    "                passes=10)\n",
    "\n",
    "model.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation with pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "pyLDAvis.gensim.prepare(model, term_document_matrix, id2word, mds='mmds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic coherence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the effect of the number of passes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "good_model = LdaModel(corpus=term_document_matrix, id2word=id2word, passes=50, num_topics=10)\n",
    "bad_model = LdaModel(corpus=term_document_matrix, id2word=id2word, passes=1, num_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_score = CoherenceModel(model=good_model, texts=corpus, dictionary=id2word)\n",
    "bad_score = CoherenceModel(model=bad_model, texts=corpus, dictionary=id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_score.get_coherence(), bad_score.get_coherence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What's the best number of topics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "scores = []\n",
    "\n",
    "for n in range(5, 10):\n",
    "    print(\"Training model with n={} topics\".format(n))\n",
    "    model = LdaModel(corpus=term_document_matrix, id2word=id2word, passes=10, num_topics=n)\n",
    "    score = CoherenceModel(model=model, texts=corpus, dictionary=id2word)\n",
    "    models.append(model)\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_topics = range(5, 10)\n",
    "coherence_scores = [s.get_coherence() for s in scores]\n",
    "\n",
    "plt.plot(n_topics, coherence_scores)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_scores\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - Train model with only nouns and adjectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part-of-speech (PoS) tagging is the process of assigning words to their grammatical categories.\n",
    "\n",
    "We can achieve this using `nltk.pos_tag()`, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "sentence = \"The quick brown fox jumped over the lazy dog\".split()\n",
    "\n",
    "pos_tag(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the function `nltk.pos_tag()` uses the set of tags from the [Penn Treebank project](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html).\n",
    "\n",
    "#### How to change the pre-processing steps to include only nouns and adjectives?\n",
    "\n",
    "#### Does this produce better topic models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution here\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
